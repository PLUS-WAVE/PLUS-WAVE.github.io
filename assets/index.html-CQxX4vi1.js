import{_ as t,c as s,e as a,o as l}from"./app-CGJJd-cj.js";const e={};function n(r,i){return l(),s("div",null,i[0]||(i[0]=[a(`<h2 id="_1-motivation" tabindex="-1"><a class="header-anchor" href="#_1-motivation"><span>1 Motivation</span></a></h2><p><a href="https://arxiv.org/abs/2312.14135" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2312.14135</a></p><blockquote><p><strong>V*</strong> 感觉就类似于 agent</p></blockquote><p>当前的多模态大模型（比如 LLaVA、GPT-4V 等）在处理复杂或高分辨率图像时，存在两个主要问题：</p><ol><li>视觉信息获取不足：图像编码器（如 CLIP）通常对低分辨率图像训练，导致在<strong>高分图像</strong>中容易忽略细节。</li><li>缺乏主动搜索能力：模型不会像人一样“知道自己不知道”，也不会主动查找<strong>关键视觉信息</strong>。</li></ol><p><strong>V</strong>*：引入了一个 LLM 驱动的视觉搜索模块 <strong>V*</strong>，模仿人类的视觉搜索方式</p><ol><li><strong>Show</strong>：VQA LLM先尝试回答问题 <ul><li>如果能回答，就直接结束</li><li>如果回答不了，它就列出<strong>缺失的关键信息</strong>（如：需要找到“红色杯子”）</li></ul></li><li><strong>Search（V*启动）</strong>：如果无法回答，就启动 <strong>V*</strong>，根据上下文、常识和语言模型的推理来定位图像中的关键区域</li><li><strong>Tell</strong>：搜索到的所有信息收集到“视觉工作记忆（VWM）”，再次输入给LLM，从而更准确地回答问题。</li></ol><img src="https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2025-05-07/image-20250426165324942.png" alt="image-20250426165324942" style="zoom:50%;"><h2 id="_2-method" tabindex="-1"><a class="header-anchor" href="#_2-method"><span>2 Method</span></a></h2><p>提出了一个新的 MLLM 框架，叫做 SEAL，全称是 Show, Search, and Tell。它的基本思路是模拟人类在看复杂图片时，会主动搜索关键细节、记在脑子里（视觉工作记忆 VWM），然后基于这些信息做更好的推理和回答。</p><h3 id="_2-1-seal" tabindex="-1"><a class="header-anchor" href="#_2-1-seal"><span>2.1 SEAL</span></a></h3><p>整个SEAL系统有两个核心部分：</p><ol><li>VQA LLM</li><li>Visual Search Model</li></ol><p>它们通过一个叫做 <strong>Visual Working Memory (VWM)</strong> 的结构来互相协作。</p><blockquote><p>VWM就是一个不断收集、更新信息的小本子，里面记录了原图、问题、找到的目标小图、目标位置。</p></blockquote><ol><li>第一步，VQA LLM 先看图 I 和问题 T，尝试回答。 <ul><li>如果能回答，就直接结束；</li><li>如果回答不了，它就列出<strong>缺失的关键信息</strong>（比如：需要找到“红色杯子”、“猫的眼睛”这些小目标）。</li></ul></li><li>第二步，初始化 VWM，把原图 I 和问题 T 加进去。</li><li>第三步，针对每一个缺失的目标： <ul><li>用 <strong>Visual Search Model</strong> 去整个图片里搜目标（用一个优先队列 q 来管理搜索过程）；</li><li>如果找到了，就把对应的小图裁剪出来，加到 VWM；</li><li>如果找不到，就在 VWM 中记录“没找到”。</li></ul></li><li>最后，基于丰富了的 VWM 信息，VQA LLM 再重新生成最终回答。</li></ol><h3 id="_2-2-数据构建" tabindex="-1"><a class="header-anchor" href="#_2-2-数据构建"><span>2.2 数据构建</span></a></h3><p><strong>负样本数据（100k）</strong></p><p>让模型学会识别：“我回答不了问题，因为图里没有这个目标”。模型要能明确地说出：“我需要 A 和 B 才能回答。”</p><p>图像中真的没有相关目标；或者目标太小（&lt; 20x20像素），CLIP提不出来特征；</p><p>构造方式：</p><ul><li>用 GPT-3.5 生成和目标物相关的问题；</li><li>用 COCO2017 图片；</li></ul><hr><p><strong>VQA数据（167k）</strong></p><p>让模型在已有目标的基础上回答问题。</p><ul><li>GQA 数据（70k）：使用 GT 标注的目标物，作为 VWM 的目标输入；用 GPT-3.5 把简短回答扩展成完整句子。</li><li>Object Attribute 数据（51k）：用 VAW 数据集（关于物体颜色、材质等）；把描述性信息变成问答格式，提取出相关物体作为目标。</li><li>Spatial Relationship 数据（46k）：在 COCO2017 上构造两个物体之间的空间关系问题（如“A在B的左边吗？”）；这两个物体就是搜索目标。</li></ul><hr><p><strong>LLaVA 指令微调数据（120k）</strong></p><p>维持模型的通用指令能力：</p><ul><li>使用 LLaVA-80K 中的图文指令数据（图片主要来自 COCO）；</li><li>再额外挑选40k条：从问题中提取出能匹配COCO类别的目标，作为搜索对象。</li></ul><h3 id="_2-3-v-🌟" tabindex="-1"><a class="header-anchor" href="#_2-3-v-🌟"><span>2.3 V* 🌟</span></a></h3><p>视觉搜索 Visual Search 跟指代理解（REC）很像：都是给一句文本描述，在图里找对应的目标。但视觉搜索要更灵活：</p><ul><li>支持<strong>任意分辨率</strong>的大图（不仅仅是标准尺寸图片）；</li><li>有时候需要<strong>在整张图中彻底搜索</strong>；</li><li>搜索效率很重要，要尽可能<strong>又快又准</strong>地找到目标。</li></ul><h4 id="_2-3-1-model-structure" tabindex="-1"><a class="header-anchor" href="#_2-3-1-model-structure"><span>2.3.1 Model Structure</span></a></h4><p>总体设计理念：</p><ul><li>模仿人类视觉搜索：<strong>先大致推测哪里可能有目标，再细看细找</strong>；</li><li>不是死遍历（暴力patchify）；</li><li>引入一个多模态大模型（MLLM） + 局部定位模块。</li></ul><p>组件：</p><ul><li><strong>MLLM</strong>（多模态语言模型）： <ul><li>输入图片和搜索指令：“Please locate the [object] in the image.”</li><li>输出一个特殊token <code>&lt;LOC&gt;</code>，包含位置相关的上下文特征；</li><li>基于 <code>&lt;LOC&gt;</code> 嵌入，拿到两个向量： <ul><li><strong>vtl</strong> → 给目标定位</li><li><strong>vcl</strong> → 给搜索提示</li></ul></li></ul></li><li><strong>Image Encoder + 两个Decoder</strong>： <ul><li><strong>Dtl</strong>（Target Localization Decoder）：类似两个MLP head，预测坐标和置信度。</li><li><strong>Dcl</strong>（Search Cue Localization Decoder）：类似 SAM 的掩码分割头，输出热力图，指示可能的目标区域。</li></ul></li></ul><h4 id="_3-3-2-search-algorithm🌟" tabindex="-1"><a class="header-anchor" href="#_3-3-2-search-algorithm🌟"><span>3.3.2 Search Algorithm🌟</span></a></h4><p>V* 搜索过程大致是：</p><ol><li><p>直接定位：</p><ul><li>用“Please locate [object]”指令</li><li>如果目标坐标置信度高 → 成功找到</li></ul></li><li><p>检查热力图：</p><ul><li>如果目标置信度低，看 Search Cue 热力图</li><li>如果热力图中有显著区域（最大值超过阈值 δ）→ 用来引导下一步搜索</li></ul></li><li><p>使用上下文推断（contextual cue）：如果热力图也不明显</p><ul><li>询问 MLLM：“目标最可能出现在图中的哪个区域？”</li><li>再基于上下文区域生成新的 Search Cue Heatmap</li></ul></li><li><p>递归图像分割搜索：</p><img src="https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2025-05-07/image-20250426170622718.png" alt="image-20250426170622718" style="zoom:50%;"><ul><li>把图像递归地按四块划分（根据图像长宽比例调整，保持patch接近正方形）；</li><li>基于热力图的优先级，按分数高的子图优先搜索；</li><li>直到找到目标或patch小到不能再切</li></ul></li></ol><p>搜索过程 example：一行代表一个过程，右边就是热力图和最后的 bbox</p><img src="https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2025-05-07/image-20250426193406342.png" alt="image-20250426193406342" style="zoom:50%;"><h3 id="_2-4-model-training" tabindex="-1"><a class="header-anchor" href="#_2-4-model-training"><span>2.4 Model Training</span></a></h3><p>主要包括两个模型：</p><ul><li><strong>VQA 模型</strong>（用于理解问题、定位目标）</li><li><strong>视觉搜索模型</strong>（用于生成热图和具体定位）</li></ul><h4 id="_2-4-1-vqa-模型训练" tabindex="-1"><a class="header-anchor" href="#_2-4-1-vqa-模型训练"><span>2.4.1 VQA 模型训练</span></a></h4><p>使用基础模型：<strong>Vicuna-7B-1.3</strong></p><p>训练分两阶段：</p><ul><li><strong>特征对齐阶段</strong>：冻结 vision encoder 和 LLM，仅训练两个投影模块（linear projection / resampler），图像-文本对用的是 LLaVA 用的 558K LAIONCC-SBU 子集。</li><li><strong>指令微调阶段</strong>：冻结 vision encoder，训练 Vicuna 和 projection 模块，使用构建的 387K 任务数据</li></ul><p>推理输入格式（最终喂给 LLM）：</p><div class="language-css line-numbers-mode" data-highlighter="shiki" data-ext="css" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">&lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">Image</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> </span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Additional visual information:</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">{</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">Object</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;"> name</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> 1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">}</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">Object</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> at </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x1, y1, x2, </span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">y2</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">]</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">{</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">Object</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;"> name</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> 2</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">}</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">Object</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> at </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x1, y1, x2, </span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">y2</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">]</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">;</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">...</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Question</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><code>&lt;Image&gt;</code> 和 <code>&lt;Object&gt;</code> 都是通过 projection 得到的图像/目标的 token。</li><li>若只有一个目标，就用 linear proj；否则用 resampler 来多目标聚合。</li></ul><h4 id="_2-4-2-视觉搜索模型训练" tabindex="-1"><a class="header-anchor" href="#_2-4-2-视觉搜索模型训练"><span>2.4.2 视觉搜索模型训练</span></a></h4><p>模型结构：</p><ul><li>MLLM：LLaVA-7B-v1.1</li><li>Vision encoder：OWL-ViT-B-16</li><li>包含两个模块： <ul><li><strong>Dcl</strong>：Dense cue localization module → 输出热图 <ul><li>用 BCE loss + Dice loss 训练</li></ul></li><li><strong>Dtl</strong>：Discrete target localization module → 输出具体目标 box <ul><li>类似 DETR，用 set prediction loss + focal loss</li></ul></li></ul></li></ul><p>训练设置：</p><ul><li>总步数 100K，batch size 64，lr=1e-4</li><li>数据采样比例：General detection/segmentation:Referring:VQA = 15:8:15</li><li>参数冻结与可训练策略： <ul><li>冻结：image encoder（视觉骨干）、Dtl 中的坐标 MLP</li><li>可训练：MLLM（用 LoRA）、word embedding、Dcl、score MLP</li></ul></li></ul>`,58)]))}const p=t(e,[["render",n]]),h=JSON.parse(`{"path":"/article/9ewtfoxo/","title":"Vstar 学习笔记","lang":"zh-CN","frontmatter":{"title":"Vstar 学习笔记","tags":["CV","VLM","Reasoning"],"createTime":"2025/05/07 09:49:33","permalink":"/article/9ewtfoxo/","cover":"https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2025-05-07/image-20250426165324942.png","description":"1 Motivation https://arxiv.org/abs/2312.14135 V* 感觉就类似于 agent 当前的多模态大模型（比如 LLaVA、GPT-4V 等）在处理复杂或高分辨率图像时，存在两个主要问题： 视觉信息获取不足：图像编码器（如 CLIP）通常对低分辨率图像训练，导致在高分图像中容易忽略细节。 缺乏主动搜索能力：模型不会...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Vstar 学习笔记\\",\\"image\\":[\\"https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2025-05-07/image-20250426165324942.png\\"],\\"dateModified\\":\\"2025-05-07T02:19:51.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://plus-wave.github.io/article/9ewtfoxo/"}],["meta",{"property":"og:site_name","content":"PLUS-WAVE's Blog"}],["meta",{"property":"og:title","content":"Vstar 学习笔记"}],["meta",{"property":"og:description","content":"1 Motivation https://arxiv.org/abs/2312.14135 V* 感觉就类似于 agent 当前的多模态大模型（比如 LLaVA、GPT-4V 等）在处理复杂或高分辨率图像时，存在两个主要问题： 视觉信息获取不足：图像编码器（如 CLIP）通常对低分辨率图像训练，导致在高分图像中容易忽略细节。 缺乏主动搜索能力：模型不会..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2025-05-07/image-20250426165324942.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-05-07T02:19:51.000Z"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2025-05-07/image-20250426165324942.png"}],["meta",{"name":"twitter:image:alt","content":"Vstar 学习笔记"}],["meta",{"property":"article:tag","content":"Reasoning"}],["meta",{"property":"article:tag","content":"VLM"}],["meta",{"property":"article:tag","content":"CV"}],["meta",{"property":"article:modified_time","content":"2025-05-07T02:19:51.000Z"}]]},"readingTime":{"minutes":6.14,"words":1842},"git":{"updatedTime":1746584391000,"contributors":[{"name":"PLUS_WAVE","username":"","email":"wangplus_wave@foxmail.com","commits":1,"avatar":"https://gravatar.com/avatar/73d9cce6b7473bc4e3bccd9c674dc373250f563551d205366d1b3852d719f74e?d=retro"}]},"autoDesc":true,"filePathRelative":"2. CV/27. Vstar.md","headers":[],"categoryList":[{"id":"de90e8","sort":2,"name":" CV"}]}`);export{p as comp,h as data};
