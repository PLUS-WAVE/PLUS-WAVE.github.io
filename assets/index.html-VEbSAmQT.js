import{_ as t,c as o,e as r,o as i}from"./app-CGJJd-cj.js";const a={};function n(l,e){return i(),o("div",null,e[0]||(e[0]=[r('<h2 id="_1-overview" tabindex="-1"><a class="header-anchor" href="#_1-overview"><span>1 Overview</span></a></h2><p><a href="https://arxiv.org/abs/2501.13926" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2501.13926</a></p><p><strong>将 CoT 推理策略从语言任务迁移到自回归图像生成中</strong></p><p>自回归图像生成与语言生成在“逐token生成”的机制上高度相似，为应用CoT提供了潜在可行性</p><blockquote><p>Show-o 的图像生成方式本质上是一种<strong>基于 mask 的 iterative denoising 机制</strong>，但操作在 <strong>token 层级</strong> 而不是 pixel 层级</p><ul><li>每一轮输出都能还原出一张中间图像（虽然部分 token 还是 mask）</li></ul><p>👀但是看下来，跟 Autoregressive 的的范式没什么关系，Diffusion 也能这样做吧；这更应该归为<strong>通用生成中的 reward 引导与路径优化机制</strong></p><p>更新：已经有用 GRPO 做出类似想法的工作了：<a href="https://www.arxiv.org/abs/2505.05470" target="_blank" rel="noopener noreferrer">Flow-GRPO</a></p></blockquote><img src="https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2025-05-05/image-20250505110137747.png" alt="image-20250505110137747" style="zoom:50%;"><p>探索了两大类增强策略：</p><ul><li><p><strong>测试时验证（test-time verification）</strong>：利用Reward Model（ORM 和 PRM）挑选生成结果</p><ul><li>ORM 显著提升结果，而PRM效果有限</li></ul></li><li><p><strong>偏好对齐（preference alignment）</strong>：利用DPO（Direct Preference Optimization）优化生成策略</p><ul><li>DPO 对齐优于仅使用 Reward Model</li></ul><blockquote><p>当时还没有 GRPO 改进 ORM/PRM：</p></blockquote></li><li><p><strong>PARM</strong>：三步式潜力评估机制，解决 PRM 早期图模糊与 ORM 全局评估不准的问题</p></li><li><p><strong>PARM++</strong>：在 PARM 基础上引入反思机制，对生成失败图像进行自我纠错与重生成</p></li></ul><p>使用 <strong>Show-o</strong> 作为主干</p><h2 id="_2-method" tabindex="-1"><a class="header-anchor" href="#_2-method"><span>2 Method</span></a></h2><img src="https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2025-05-05/image-20250505110156944.png" alt="image-20250505110156944" style="zoom:50%;"><h3 id="_2-1-dpo-orm" tabindex="-1"><a class="header-anchor" href="#_2-1-dpo-orm"><span>2.1 DPO+ORM</span></a></h3><p>具体做法：</p><p><strong>ORM</strong>：只在生成完成后评估图像是否符合 prompt —— 聚焦最终结果，图像清晰，语义完整，评估准确</p><ol><li>使用预训练的 LLaVA-OneVision-7B 作为 Zero-shot ORM <ul><li>通过 prompt template 输入“文字+生成图” → 模型输出 “yes”/“no”</li><li>用“yes”概率最大的作为最终输出（best-of-N）</li></ul></li><li>构造 288K 的图文对比数据集（使用 GPT-4 生成 prompt，Show-o 生成图像，人工/自动打标签）进行 fine-tune → 提升 ORM 判断能力</li></ol><p><strong>PRM</strong>：在生成过程中的每一步图像都进行打分 —— 前期图像模糊，难评估；后期图像差异小，难区分。</p><ol><li>同样从 LLaVA-OneVision 开始 zero-shot</li><li>构造 1 万个 step-wise 的图文序列做精调</li></ol><p><strong>DPO</strong>：基于之前用于训练 ORM 的“好图/坏图”二分类数据，构建了 DPO 所需的偏好排序对（preferred/dispreferred pairs）</p><ul><li>policy model：从 Show-o 初始化，并在训练中更新</li><li>reference model：也来自 Show-o，但保持冻结</li><li>优化目标：鼓励策略模型对更优图像（preferred）赋予更高概率</li></ul><p><strong>DPO+ORM：1 &lt; 2 &lt; 3</strong></p><ol><li>DPO + ORM guidance：ORM 为 DPO 时 reward model（DPO 训练时，不仅使用 ranking pairs 进行 loss 计算，还引入 ORM 的输出作为奖励信号）</li><li>DPO + test-time ORM：普通 DPO 后，再用 ORM 在推理时做 best-of-N 策略挑选</li><li>DPO with guidance + test-time ORM：训练时就引入 ORM 做 reward guidance，同时推理时也使用 ORM 做筛选（最好的）</li></ol><h3 id="_2-2-parm-和-parm" tabindex="-1"><a class="header-anchor" href="#_2-2-parm-和-parm"><span>2.2 PARM 和 PARM++</span></a></h3><p>PARM = reward + filtering；PARM++ = reward + filtering + self-correction</p><p>PARM：</p><ol><li><p>Clarity Judgment：</p><ul><li>在生成的每一步，判断当前图像是否足够清晰，如果太模糊就跳过评分</li><li>解决了 PRM early-stage 图像没法评判的问题</li></ul></li><li><p>Potential Assessment：</p><ul><li>对通过 clarity 判断的步骤，做二值潜力判断：这个中间状态有没有可能发展成高质量结果？</li><li>如果潜力低，<strong>直接截断该路径</strong>（early termination）</li></ul></li><li><p>Best-of-N′ Selection</p><ul><li><p>在所有剩下的 high-potential 路径中（数量为 N′），做一个类似 ORM 的最终打分选最优</p></li><li><p>如果没有路径通过，则选 “拒绝最多次”的路径（最不被否定的）</p></li></ul></li></ol><p>PRM 太早评分 → 噪声；ORM 太晚评分 → 粗粒度，不可控</p><p>而 PARM 是中间可插入型 reward：既能 early exit 又能 final pick</p><hr><img src="https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2025-05-05/image-20250505110236712.png" alt="image-20250505110236712" style="zoom:50%;"><p>PARM++：</p><ol><li><p>Reflection Evaluation：由 PARM++ 来判断最终图像是否与文本 prompt 对齐：</p><ul><li>若对齐：返回 “yes” → 成品图像</li><li>若不对齐：返回 “no” 并附上文本说明<strong>哪里错了</strong>（如颜色错误、布局问题）</li></ul><p>模拟人类<strong>自我检查</strong>：做完图再看一眼，“这和我要求的像吗？哪里出错？”</p></li><li><p>Self-Correction：输入三样东西给图像生成模型：</p><ul><li>原始文本 prompt</li><li>上一轮失败的图像</li><li>反思输出的错误分析（文本）</li></ul><p>然后模型尝试根据反馈重新生成图像，最多迭代 <strong>3 轮</strong></p></li><li><p>为修正机制专门训练模型：因为 Show-o 一开始并不会“根据文本纠正图像”，所以额外训练了 self-correction 能力：</p><ul><li><p>训练数据来自 PARM++：包含（text, 差图, 好图, 错误分析）四元组</p></li><li><p>用这批数据微调 Show-o，让它学会根据错误提示优化生成质量</p></li></ul></li></ol><blockquote><p>📌 DPO 与 PARM++ 的兼容性问题：</p><p>在 PARM++ 实验中<strong>没有使用 DPO 对齐</strong>，原因是 self-correction 微调和 DPO 的目标可能<strong>冲突</strong>，如果同时做，可能会导致训练干扰或过拟合到某一目标</p><ul><li><strong>DPO 是 preference alignment</strong>：目标是让模型输出“人更喜欢”的图像</li><li><strong>PARM++ 的 self-correction 微调</strong>：目标是让模型“听懂改图建议并动手改图”</li></ul></blockquote><p>PARM++ 开启反思机制后，相比 PARM 提高了 <strong>+10% GenEval 分数</strong></p><p>微调后的 Show-o 在原始 GenEval 上略有下降 <strong>-2%</strong></p><ul><li>说明模型在学习自我修正能力时略微牺牲了原始泛化性（但可接受）</li></ul><h3 id="_2-3-适用建议" tabindex="-1"><a class="header-anchor" href="#_2-3-适用建议"><span>2.3 适用建议</span></a></h3><ul><li>如果目标是<strong>一次性生成最优图像</strong> → <code>DPO + ORM</code> 或 <code>PARM</code></li><li>如果目标是<strong>质量控制 + 迭代优化图像内容</strong> → <code>PARM++</code> 更优，尤其适合需要多轮 refinement 的任务</li></ul>',37)]))}const p=t(a,[["render",n]]),g=JSON.parse(`{"path":"/article/n98yl04i/","title":"Can We Generate Images with CoT? 学习笔记","lang":"zh-CN","frontmatter":{"title":"Can We Generate Images with CoT? 学习笔记","tags":["CV","Generation","Reasoning"],"createTime":"2025/05/05 16:23:38","permalink":"/article/n98yl04i/","cover":"https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2025-05-05/image-20250505110137747.png","description":"1 Overview https://arxiv.org/abs/2501.13926 将 CoT 推理策略从语言任务迁移到自回归图像生成中 自回归图像生成与语言生成在“逐token生成”的机制上高度相似，为应用CoT提供了潜在可行性 Show-o 的图像生成方式本质上是一种基于 mask 的 iterative denoising 机制，但操作在 t...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Can We Generate Images with CoT? 学习笔记\\",\\"image\\":[\\"https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2025-05-05/image-20250505110137747.png\\"],\\"dateModified\\":\\"2025-05-11T07:49:35.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://plus-wave.github.io/article/n98yl04i/"}],["meta",{"property":"og:site_name","content":"PLUS-WAVE's Blog"}],["meta",{"property":"og:title","content":"Can We Generate Images with CoT? 学习笔记"}],["meta",{"property":"og:description","content":"1 Overview https://arxiv.org/abs/2501.13926 将 CoT 推理策略从语言任务迁移到自回归图像生成中 自回归图像生成与语言生成在“逐token生成”的机制上高度相似，为应用CoT提供了潜在可行性 Show-o 的图像生成方式本质上是一种基于 mask 的 iterative denoising 机制，但操作在 t..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2025-05-05/image-20250505110137747.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-05-11T07:49:35.000Z"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2025-05-05/image-20250505110137747.png"}],["meta",{"name":"twitter:image:alt","content":"Can We Generate Images with CoT? 学习笔记"}],["meta",{"property":"article:tag","content":"Reasoning"}],["meta",{"property":"article:tag","content":"Generation"}],["meta",{"property":"article:tag","content":"CV"}],["meta",{"property":"article:modified_time","content":"2025-05-11T07:49:35.000Z"}]]},"readingTime":{"minutes":4.86,"words":1459},"git":{"updatedTime":1746949775000,"contributors":[{"name":"PLUS_WAVE","username":"","email":"wangplus_wave@foxmail.com","commits":5,"avatar":"https://gravatar.com/avatar/73d9cce6b7473bc4e3bccd9c674dc373250f563551d205366d1b3852d719f74e?d=retro"}]},"autoDesc":true,"filePathRelative":"2. CV/25. Can We Generate Images with CoT.md","headers":[],"categoryList":[{"id":"de90e8","sort":2,"name":" CV"}]}`);export{p as comp,g as data};
