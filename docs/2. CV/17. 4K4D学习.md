---
title: 4K4D学习笔记
tags: 
  - 3DVision
  - 3D/4D Reconstruction
createTime: 2024/05/12 16:30:25
permalink: /article/fw9aottl/
cover: https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2024-05-12/image-20240512092301659.png
---

## 1 Pipeline

### 1.1 特征向量的计算

使用空间雕刻算法从多视图视频中得到粗糙**点云**，利用 K-Planes 使用的方法，定义六个特征平面$θxy\text{、} θxz\text{、}θyz\text{、}θtx\text{、}θty\text{、}θtz$ ，来模拟一个4D特征场 $Θ(x, t)$ ，为每个点分配一个**特征向量 $f$** <!-- more -->

对于4D空间中的任意点 $(x, t)$ ，将6个平面的特征向量拼接起来形成一个完整的特征向量 $f$

1. **空间位置信息**：通过θxy, θxz, θyz特征平面，可以获得点在3D空间中的位置相关信息
2. **时间信息**：通过θtx, θty, θtz特征平面，可以获得点随时间变化的信息

<img src="https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2024-05-12/image-20240512092301659.png" alt="image-20240512092301659" style="zoom: 67%;" />

### 1.2 几何建模

- 点位置的建模

  点的位置是通过一个**可优化**的向量 $p \in \mathbb{R}^3$ 来表示

- 点半径和密度的预测

  点的半径 $r$ 和密度 $σ$ 是使用 MLP 根据特征向量 $f$ 来预测

### 1.3 外观建模⭐

同样使用特征向量 $f$ ，进一步预测外观属性，即颜色和光照

外观模型由两部分组成：

- 离散视角依赖的外观 $c_{ibr}$（通过图像融合技术获得）
- 连续视角依赖的外观 $c_{sh}$（使用球谐函数SH模型表示）

#### 1） 球谐函数SH模型

> 球谐函数用于表示颜色：球谐函数用于颜色的基本思路是将环境光照分解为球谐基函数的系数。这些系数随后可以用来重建光照，从而计算物体表面在特定方向上的颜色。

$$
c_{\text{sh}}(s, d)
$$

$c_{sh}$ 是使用球谐函数表示的颜色，其中 $s$ 是点 $x$ 在时间 $t$ 的SH系数，$d$​ 是观察方向。

SH模型用于表示动态场景中点的颜色随着观察方向的变化而变化。每个点的颜色 $c_{sh}$ 由一组SH系数 $𝑠$ 来描述，这些系数**通过MLP网络**从特征向量 $𝑓$ 中回归得到。

#### 2） 图像融合技术

1. **投影到输入图像中**：对于动态场景中的每个点 $𝑥$ ，首先使用相机的内参将其投影回输入图像中，提取RGB颜色值 $c_{i, img}$ 。
2. **计算融合权重**：然后，根据点的坐标和所在的输入图像，计算对应的融合权重 $w_i$ 。这个融合权重会考虑点与输入图像中像素的距离等因素，以确定对最终颜色的贡献程度。注意：这里的融合权重与**观察方向无关**。
3. **选择最近的输入视角**：为了实现视角相关的效果，根据观察方向通过最近邻检索获得选择最近的 $N'$​ 个输入视角。
4. **颜色融合**：最后，根据选定的 $N'$ 个输入视角和对应的融合权重，对输入的RGB颜色进行加权融合，得到最终的颜色 $c_{ibr}$。这样，每个点在特定视角下的颜色就被融合出来了。

$$
c_{ibr}(x, t, d) = \sum_{i=1}^{N'} w_i c_{i, img}
$$

其中，$c_{ibr}$ 是点 $x$ 在时间 $t$ 和观察方向 $d$ 下的颜色，$N'$ 是根据观察方向选择的最近输入视图的数量，$w_i$ 是基于点坐标和输入图像计算的混合权重，$c_{i, img}$ 是输入图像中的RGB颜色。

其是由有限数量的颜色加权组合而成，导致在视角方向上存在离散性。结合球谐函数（SH）模型来精细地调整颜色，使其在不同视角之间平滑过渡。

$$
c(x, t, d) = c_{ibr}(x, t, d) + c_{sh}(s, d)
$$



### 1.4 可微分深度剥离渲染

> **深度剥离**：一种顺序无关半透明渲染算法，这一方案的基本思想是先进行半透明片元绘制（这里是通道而不是实际绘制），并以深度顺序排入一个多层深度缓冲结构中；渲染时在基于这个多层深度缓冲进行从远到近的渲染，就能逐层正确混合了。

由于点云表示的优势，可以利用硬件光栅化器显著加快深度剥离过程（即一开始的半透明片元绘制）。此外，这个渲染过程易于变为可微分的，使得可以从输入的RGB视频中进行模型学习。

自定义着色器渲染：

1. 深度剥离（对于特定的图像像素 $u$ ）

   1. 首先使用硬件光栅化器将点云渲染到图像上，将最靠近相机的一个点 $x_0$ 分配给像素 $u$ 。然后，记录下该点的深度 $t_0$ 。
   2. 在第 $k$ 次渲染通道中，将所有深度值 $t_k$ 小于前一通道记录的深度 $t_{k-1}$ 的点云的点都丢弃，从而得到每个像素的第 $k$ 个最靠近相机的点 $x_k$ 。在K次渲染后，像素 $u$ 就有了一组以深度排序的点 $\{x_k|k = 1, …, K\}$ 。

2. **密度权重**：基于每个像素上的点集 $\{x_k|k = 1, …, K\}$ ，使用体渲染技术来合成像素的颜色。

   对于每个点 $x_k$ ，其对像素 $u$ 的密度权重为 $\alpha_k$ ：

   $$
   \alpha_k(u, x) = \sigma \cdot \max(1 - \frac{\lVert \pi(x) - u \rVert^2}{r^2}, 0)
   $$

   这里，$\pi$ 是点 $x$ 到像素 $u$ 的投影函数，$\sigma$ 和 $r$ 分别是点的密度和半径。

3. **颜色合成**：最终像素的颜色是根据渲染通道中每个点的颜色和密度进行加权融合得到的：
   $$
   C(u) = \sum_{k=1}^{K} T_k \alpha_k c_k
   $$

   其中，$T_k$ 表示从相机到像素 $u$ 的路径上，由前 k-1 个点累积起来的透明度：

   $$
   T_k = \prod_{j=1}^{k-1} (1 - \alpha_j)
   $$

整个密度加权过程是可微分的，这意味着可以通过反向传播来更新点云的属性，以最小化渲染图像和真实图像之间的差异。

## 2 Train（loss）

1. **图像重建损失**：首先，使用均方误差（MSE）损失来比较生成的图像像素颜色 $C(u)$ 与真实图像像素颜色 $C_{gt}(u)$ 之间的差异。
   $$
   L_{\text{img}} = \sum_{u \in U} ||C(u) - C_{\text{gt}}(u)||_2^2
   $$

2. **感知损失**：应用来提高生成图像的质量。感知损失通过比较生成图像和真实图像在VGG16网络中提取的图像特征之间的差异来度量图像之间的感知差异。
   $$
   L_{\text{lpips}} = ||\Phi(I) - \Phi(I_{\text{gt}})||_1
   $$

   其中， $Φ$ 是一个VGG16网络， $I$ 和 $I_{gt}$​ 分别是生成图像和真实图像。

   > 感知损失是**一种基于神经网络特征的损失函数**，它通过比较目标图像和生成图像在高层特征图上的差异来度量图像的相似性。这种损失函数能够更好地捕捉图像的语义信息，从而使得生成图像更符合人类的视觉感知。 在PyTorch中，我们可以使用预训练的神经网络（如VGG或ResNet）提取特征，然后计算特征图之间的差异。

3. **掩模损失**：应用了掩模损失。

   仅渲染动态区域的点云以获取它们的掩码（即透明度）：

   $$
   M(u) = \sum_{k=1}^{K} T_k \alpha_k
   $$

   将生成的掩模与真实的掩模进行比较，以限制动态区域的几何形状，确保其与真实场景中的可视外壳一致。
   $$
   L_{\text{msk}} = \sum_{u \in U'} M(u)M_{\text{gt}}(u)
   $$

   其中， $U'$ 表示渲染掩模的像素集， $M_{gt}$ 是2D动态区域的真实掩模。通过引入掩模损失，模型可以受到动态区域几何形状的有效约束。（**正则化**）

最终的损失函数由图像重建损失、感知损失和掩模损失的加权和组成：

$$
L = L_{\text{img}} + \lambda_{\text{lpips}} L_{\text{lpips}} + \lambda_{\text{msk}} L_{\text{msk}}
$$

其中， $λ_{lpips}$和 $λ_{msk}$ 是控制相应损失权重的超参数。



### 2.1 具体的 Mask Loss

Mask Loss 在 `MaskSupervisor` 类中被计算。

#### 1） mIoU Loss

`mIoU_loss`函数计算的是交并比（Intersection over Union, IoU）损失。表示预测和真实分割区域的交集与并集的比值。**交并比越大，预测掩码和真实掩码的重叠程度越高，所以损失应该越小**。

计算步骤：

1. 计算交集 \( I \)：预测值和真实值逐元素相乘，再求和： $I = \sum (x \cdot y)$
2. 计算并集 \( U \)：预测值和真实值逐元素相加，再减去交集的值： $U = \sum (x + y) - I$
3. 计算mIoU：交集除以并集，然后取反得到损失： $\text{mIoU\_loss} = 1- \text{mIoU} = 1 - \frac{I}{U}$

#### 2） MSE Loss

`mse`函数计算的是均方误差损失： $\text{mse} = \frac{1}{N} \sum (x_i - y_i)^2$

#### 3） 总损失计算

在`compute_loss`函数中，`loss`计算涉及到两个部分，总损失 `loss` 是在已有损失基础上增加了上述两种损失的加权和。：

- `mIoU_loss`部分，具体权重为 `self.msk_loss_weight` 。
- `mse`部分，具体权重为 `self.msk_mse_weight` 。

总损失公式如下：

$$
L_{mask} =  w_{mIoU} \cdot L_{mIoU} + w_{mse} \cdot L_{mse}
$$

其中：

$$
L_{mIoU} = 1 - \frac{\sum (x \cdot y)}{\sum (x + y) - \sum (x \cdot y)} 
$$

$$
L_{mse} = \frac{1}{N} \sum (x_i - y_i)^2
$$



## 3 Inference

### 3.1 预计算和预计存储

- **点位置** $p$ 、**半径** $r$ 、**密度** $\sigma$ 、**球谐系数** $s$ 以及**颜色混合权重** $w_i$ 在训练完成后，对于场景中的每个点，预先计算其在渲染过程中这些需要的属性。
- 这些属性在渲染期间会异步传输到图形卡上，内存传输操作与GPU上的光栅化过程重叠，这样可以进一步提高渲染速度。

运行时的计算量减少到只需进行**深度剥离**和 $c(x, t, d) = c_{ibr}(x, t, d) + c_{sh}(s, d)$ 

### 3.2 数据类型

将模型的数据类型从32位浮点数转换为16位浮点数，以提高内存访问效率。这种转换可以提高每秒帧数（FPS）约20帧，并且不会导致可见的性能损失。

### 3.3 减少渲染传递次数

将可微深度剥离算法的渲染传递次数 $K$ 从15减少到12。这也能提高FPS约20帧，同时不改变视觉质量。

## 4 实施细节

#### 1）超参

- 使用Adam优化器，学习率为 $5 \times 10^{-3}$，通常在单个RTX 4090 GPU上训练200帧的序列，需要大约24小时。
- 点位置的学习率设置为 $1 \times 10^{-5}$，正则化损失权重 $\lambda_{\text{lpips}}$ 和 $\lambda_{\text{msk}}$ 设置为 $1 \times 10^{-3}$。
- 在训练期间，可微深度剥离的传递次数 $K$ 设置为15，最近输入视图的数量 $N'$ 设置为4。

#### 2）点云初始化

- 对于动态区域，使用**分割方法获得输入图像中的掩码**，并使用**空间雕刻算法提取粗点云**。

- 对于静态背景区域，利用前景掩码计算所有帧中背景像素的掩码加权平均值，从而得到不包含前景内容的**背景图像**。

  然后，在这些图像上训练 Instant-NGP，从中获得初始点云。



