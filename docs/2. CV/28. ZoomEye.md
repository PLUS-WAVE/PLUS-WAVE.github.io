---
title: ZoomEye 学习笔记
tags: 
  - CV
  - VLM
  - Reasoning
createTime: 2025/05/07 09:49:33
permalink: /article/3ve7xm35/
cover: https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2025-05-07/image-20250429114613574.png
---
## 1 Motivation

[https://arxiv.org/abs/2411.16044](https://arxiv.org/abs/2411.16044)

> 本质上来说，这个工作应该就是 prompt engineering / agent，都是人为设定好的规则来 tree search 目标物体，是 training-free 的<!-- more -->
>
> 可能由于当时（2024年）模型的输入上下文长度限制，导致其无法进行类似 o3 那样的自由探索

MLLMs 由于视觉编码器输入分辨率受限（如224×224），以及视觉上下文过于密集，**容易忽视细节，只关注主要对象**。

即使像 AnyRes 这样的技术能处理高分辨率，但仍有两大问题：

1. Patch数量受限，超高分辨率图还需要下采样
2. 所有细节平等处理，导致重要细节难以重点关注

**Zoom Eye**：把图像结构化成一棵树，根节点是全图，子节点是局部细节区域，越深层代表越精细的局部。基于树的搜索算法：Zoom in（深入节点，看局部细节）；Zoom out（回溯节点，查看其它区域）

- 无训练 & 模型无关：可以直接应用到各种现成的MLLM上

流程概括：

1. 根据问题让模型初步找相关区域
2. 设计两种置信度，指导搜索顺序
3. 按置信度搜索树节点，直到能自信回答问题为止

## 2 Method

<img src="https://raw.githubusercontent.com/PLUS-WAVE/blog-image/master/img/blog/2025-05-07/image-20250429114613574.png" alt="image-20250429114613574" style="zoom: 50%;" />

树搜索：

- Tree Node 结构：每个节点有 `id`, `depth`, `value`, `children`，可以自定义更多字段。
- Tree Search定义：
  - $T$：树结构
  - $Q$：候选节点队列
  - $R$：排名函数（选最高优先的节点）
  - $S$：停止条件
- 基本流程：
  1. 初始化 $Q$ 把根节点放进去。
  2. 每次取 $Q$ 顶端节点 $n_t$，判断是否满足 $S(n_t)$，如果是就停止。
  3. 否则，把它的子节点加入 $Q$，再根据 $R$ 排序 $Q$。

例子：比如 DFS 搜索值为5的节点，$R$ 就是按 depth 优先、id 次优先排序，$S$ 是判断 value 是否为5

### 2.1 Tree Representation for Image

把**整张图像建模成一棵树**：

- 每个节点 `nt` 表示图像的一个**局部区域**（`I, bt`），`bt`是归一化的 bbox 坐标
- 如果节点的 patch 太大（超过视觉编码器的分辨率），就把它等分成4个小patch作为子节点
- 递归地细分，直到每个节点符合分辨率要求；起始节点是整张图片，`bt = (0, 0, 1, 1)`

由于高分辨率图像的信息非常密集，MLLM 一开始很难完全理解，所以需要让模型可以不断"放大、扫视"局部区域，探索更深的节点。提出两种输入方式来让 MLLM 感知局部 patch：

1. **Local Input**：只喂局部 patch（适合简单的 MLLMs）
2. **Global+Local Input**：喂整图+局部patch（适合高级 MLLMs，采用 AnyRes 处理）

### 2.2 Ranking Function

定义一个节点优先级 Ranking Function R，来控制探索顺序；用MLLM来辅助计算优先级：

- **Existing Confidence $c_e$**：这个 patch 里现在就能看到线索的概率。
- **Latent Confidence $c_l$**：如果继续 zoom，有可能找到线索的概率。

使用两个 prompt 去问 MLLM，根据输出 Yes/No 的概率的比例来作为权重：

- `pe(o)`：“这里有 o 吗？”
- `pl(o)`：“继续放大能找到 o 吗？”

两者加权求一个总优先级，权重 $W(d)$ 根据节点深度动态调整（深度小给 latent 信号更多权重，深度大给 existing 信号更多权重）

### 2.3 Stopping Criterion

定义**停止准则S**，判断当前节点是否已经足够回答问题：

- 用一个prompt `pa(qs)`问：“现在能回答问题了吗？”
- MLLM 给出 "Yes" 的概率，如果超过阈值 $\tau$ 则停止

------

### 2.4 Overall Search Algorithm

完整流程分三步：

（1）生成视觉线索（Visual Cues）：在搜索之前，MLLM 生成跟问题相关的线索（如“狗”，“猫”等目标），线索分两类：

- Type1：找**一个实例**（比如找一只狗）
- Type2：找**所有实例**（比如找所有的狗）

| 问题                                                 | 视觉线索 | 类型           |
| ---------------------------------------------------- | -------- | -------------- |
| What is the color of the dog?                        | dog      | type 1         |
| What is the relative position of the dog to the cat? | dog, cat | type 1, type 1 |
| How many dogs in the image?                          | all dogs | type 2         |

------

（2）搜索视觉线索：对每个视觉线索，构建并遍历图像树。

- 对 Type1 线索：搜到满足条件（即S成立）的节点就停
- 对 Type2 线索：把所有存在线索的节点都记录下来，不设S

------

（3）答题

- 把找到的所有相关节点的bounding box联合起来，形成一个区域
- 把这个区域（局部图像）和问题q一起输入MLLM，生成最终回答